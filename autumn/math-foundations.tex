\documentclass[oneside,reqno,letterpaper]{amsart}

\usepackage{silence} % for suppressing warnings
\WarningFilter{mdframed}{You have requested package}

\usepackage[plain]{/Users/aden/Library/CloudStorage/Box-Box/latex/adenc}



\title[AP-Lec01]{Asset Pricing Lecture 01: Mathematical Foundations}
\author{Aden Chen}



\begin{document}
\maketitle

\tableofcontents

\section{Derivatives: A One-Dimensional Recap}

\begin{definition}
  Let \(f: \RR \supset \Omega \to \RR\). 
  The derivative of \(f\) at \(x \in \Omega\) is defined as 
  \[
    f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} , 
  \] 
  if the limit exists. 
\end{definition}
\begin{remark}~
  \begin{itemize}
    \item Think ``first order approximation.''
    \item Note that \(f'\) is also a function, with the same domain as \(f\) (when \(f\) is enough regular). 
  \end{itemize}
\end{remark}



\section{Derivatives: Partial and Total}
\begin{definition}
  Let \(f: \RR^n \supset \Omega \to \RR\). 
  The partial derivative of \(f\) at \(\vec{x} \in \Omega\) with respect to the \(i\)th variable is defined as 
  \[
    \frac{\partial f}{\partial x_i}
    = f_{x_i}
    = f_i(x) 
    = \lim_{h \to 0} \frac{f(\vec{x} + h \vec{e}_i) - f(\vec{x})}{h} , 
  \] 
  if the limit exists. 
\end{definition}
\begin{remark}~
  \begin{itemize}
    \item Think derivative with respect to the \(i\)th position, not to \(x_i\). 
    \item ``First order approximation in the \(i\)th direction.``
    \item With enough regularity imposed on \(f\), we can write 
      \[
        \d f = \sum_{k = 1}^{n} \frac{\partial f}{\partial x_i} . 
      \] 
      ``Think first order approximation.''
  \end{itemize}
\end{remark}



\section{The Lagrangian}
\begin{example}
  \[
    \max_{\{x, y\}} U(x, y), \quad \text{s.t.} 
    \begin{cases}
      p_x x + p_y y \leq m \\ 
      x, y \geq 0
    \end{cases}
  \]
  \begin{itemize}
    \item For ease of mathematics, we often consider the constraint \(p_x x + p_y y = m\). 
    \item For a more general case, see \href{https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions}{Karush–Kuhn–Tucker conditions}. 
  \end{itemize}
\end{example}


\begin{theorem}
  More abstractly, the above problem can be thought of as 
  \[
    \max f(x_1, x_2) \quad \text{ s.t. } g(x_1, x_2) = c 
  \] 
  for a constant \(c\). 
  With enough regularity, the optima occurs at the critical points of the Lagrangian, defined as 
  \[
    \cL(x_1, x_2, \lambda)
    \coloneqq f(x_1, x_2) + \lambda [c - g(x_1, x_2)] . 
  \] 
  That is, the optima subject to given constraint satisfies 
  \begin{align*}
    [x_1]: && f_1(x_1^*, x_2^*) &= \lambda g_1(x_1^*, x_2^*) \\ 
    [x_2]: && f_2(x_1^*, x_2^*) &= \lambda g_2(x_1^*, x_2^*) \\ 
    [\lambda]: && g(x_1^*, x_2^*) &= c . 
  \end{align*}
\end{theorem}


\section{Taylor Expansion}
\begin{definition}
  The Taylor polynomial of degree \(n\) of the function \(f\) around point \(a\) is given by 
  \[
    f(a + x) = \sum_{k = 1}^{n} \frac{f^{(n)}(a)}{k!} \cdot x^{k} . 
  \] 
  It has the same \(k\) derivatives as \(f\). 
  Think ``\(k\)th order approximation.``
\end{definition}



\section{Probability}
\begin{definition}
  A discrete random variable \(X\) can be described by the (at most countable) values it can attain and the probability of attaining them.

  % Let \(S\) be the set of all attainable values.
  \begin{itemize}
  \item 
    The expectation of \(X\) is defined as 
    \[
      \E(X) = \sum x \cdot \P(X = x) . 
    \] 
    Think weighted average. 

  \item 
    The variance of \(X\) is defined as 
    \[
      \Var(X) = \E[(X - \E(X))^2] . 
    \] 

  \item For discrete random variables \(X\) and \(Y\), the covariance is defined by
    \[
      \Cov(X, Y) = \E[(X - \E(X)) \cdot (Y - \E(Y))] .
    \] 
  \end{itemize}
\end{definition}

\begin{proposition}~
  \begin{itemize}
    \item \(\E\) is linear. 
    \item \(\Var(X) = \E(X^2) - \E(X)^2\) and 
      \(\Var(a + bX) = b^2 \Var(X)\). 
    \item \(\Cov(X, Y) = \E(XY) - \E(X) \E(Y)\). 
  \end{itemize}
\end{proposition}


















\end{document}


